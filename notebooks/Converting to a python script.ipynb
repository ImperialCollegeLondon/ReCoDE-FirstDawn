{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to a python script "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need to do is copy all the important cells from the previous notebook into a single python file. I have collated all the important steps below. Open up an empty text file and copy everything over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#makes sure your random number generators start with the same random seed everytime it is run.\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "\n",
    "def make_model_VGG(output = 1,l_rate = 0.01, loss = 'mean_squared_error',):\n",
    "    '''\n",
    "        Creates a CNN with the VGG architecture.\n",
    "        \n",
    "    Params:\n",
    "    -------\n",
    "    \n",
    "    output: int\n",
    "        The number of output neurons.\n",
    "    l_rate: float\n",
    "        The learning rate for the given loss function.\n",
    "    loss: str \n",
    "        Loss function to use, only excepts tf loss functions\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Tensorflow sequential model.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    initializer = tf.keras.initializers.GlorotNormal()\n",
    "    \n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1),padding ='same',kernel_initializer=initializer,use_bias =False))\n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1),padding ='same',kernel_initializer=initializer,use_bias =False))\n",
    "    model.add(tf.keras.layers.BatchNormalization(beta_initializer=initializer,momentum = 0.9))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1),padding ='same',kernel_initializer=initializer,use_bias =False))\n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1),padding ='same',kernel_initializer=initializer,use_bias =False))\n",
    "    model.add(tf.keras.layers.BatchNormalization(beta_initializer=initializer,momentum = 0.9))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1),padding ='same',kernel_initializer=initializer,use_bias =False))\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1),padding ='same',kernel_initializer=initializer,use_bias =False))\n",
    "    model.add(tf.keras.layers.BatchNormalization(beta_initializer=initializer,momentum = 0.9))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(256, kernel_size=(3, 3), strides=(1, 1),padding ='same',kernel_initializer=initializer,use_bias =False))\n",
    "    model.add(tf.keras.layers.Conv2D(256, kernel_size=(3, 3), strides=(1, 1),padding ='same',kernel_initializer=initializer,use_bias =False))\n",
    "    model.add(tf.keras.layers.BatchNormalization(beta_initializer=initializer,momentum = 0.9))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(1024,kernel_initializer=initializer,use_bias =False))\n",
    "    model.add(tf.keras.layers.BatchNormalization(beta_initializer=initializer,momentum = 0.9))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(1024,kernel_initializer=initializer,use_bias =False))\n",
    "    model.add(tf.keras.layers.BatchNormalization(beta_initializer=initializer,momentum = 0.9))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(1024,kernel_initializer=initializer,use_bias =False))\n",
    "    model.add(tf.keras.layers.BatchNormalization(beta_initializer=initializer,momentum = 0.9))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(output,kernel_initializer=initializer,use_bias =False))\n",
    "\n",
    "    model.compile(loss=loss,\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate = l_rate),\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#Load in dataset\n",
    "df_dataset = pd.read_pickle(\"df_recode.gzip\",compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1})\n",
    "\n",
    "#Split into training and test sets\n",
    "df_train, df_test = train_test_split(df_dataset, test_size=.2, shuffle=True, random_state=42)\n",
    "\n",
    "#Perform preprocessing\n",
    "df_train = df_train.drop(df_train[df_train['xHI'] > 0.99].index)\n",
    "df_train = df_train.drop(df_train[df_train['xHI'] < 0.01].index)\n",
    "\n",
    "df_test = df_test.drop(df_test[df_test['xHI'] > 0.99].index)\n",
    "df_test = df_test.drop(df_test[df_test['xHI'] < 0.01].index)\n",
    "\n",
    "\n",
    "#separate into images and labels\n",
    "x_train = np.array(list(df_train['maps'].values)).reshape(len(df_train),200,200,1)\n",
    "y_train = df_train['xHI'].values\n",
    "\n",
    "x_val = np.array(list(df_test['maps'].values)).reshape(len(df_test),200,200,1)\n",
    "y_val = df_test['xHI'].values\n",
    "\n",
    "\n",
    "#Standardise datasets\n",
    "means_x = np.mean(x_train)\n",
    "stds_x = np.std(x_train)\n",
    "\n",
    "x_train = (x_train - means_x)/stds_x\n",
    "x_val = (x_val - means_x)/stds_x\n",
    "\n",
    "\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "checkpoint_path = \"training/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=False,\n",
    "                                                 verbose=1)\n",
    "\n",
    "\n",
    "#model parameters\n",
    "output_nuerons = 1\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "\n",
    "model = make_model_VGG(output_nuerons,learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "print('----------------------------------------------------------')\n",
    "print('                   Fitting Model                        ')\n",
    "print('----------------------------------------------------------')\n",
    "\n",
    "\n",
    "history = model.fit(train_x, train_y,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "pickle.dump(history.history['loss'], open( dirname+\"loss.p\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this text file as ```filename_cpu.py``` where you can substitute filename for any name of your choice. With that your script is now good to go on any cpu based cluster, Tensorflow and numpy are automatically parallalised across cpus so when you run this file, it should use all cpus you have access to.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to run this script on the Imperial HPC which would require you to write a job file to submit. I will show an example of this below, for a more in-depth tutorial on the Imperial HPC see the graduate school course: https://www.imperial.ac.uk/students/academic-support/graduate-school/students/doctoral/professional-development/research-computing-data-science/courses/introduction-to-hpc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example job file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PBS -l walltime=48:00:00\n",
    "#PBS -lselect=2:ncpus=32:mem=124gb:mpiprocs=4:ompthreads=8\n",
    "\n",
    "module load anaconda3/personal\n",
    "\n",
    "source activate tflow\n",
    "\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "python filename_cpu.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two lines asks the server for the resources you require, and the maximum time you need them for. The third line loads the anaconda package into your working environment. You would then source into your python virtual environment using conda (you would need to create this virtual environment before hand and install all python packages you would need). Finally, the penultimate line changes directory to your current working directory, which is the directory that the python script is saved in, then runs the script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With access to the Imperial HPC, you also have access to GPUs. These will greatly reduce the time it takes to train deep network models. To make use of these GPUs, we need to make a few adjustmusts to our initial script. These are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#makes sure your random number generators start with the same random seed everytime it is run.\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "\n",
    "def make_model_VGG(output = 1,l_rate = 0.01, loss = 'mean_squared_error',):\n",
    "    '''\n",
    "        Creates a CNN with the VGG architecture.\n",
    "        \n",
    "    Params:\n",
    "    -------\n",
    "    \n",
    "    output: int\n",
    "        The number of output neurons.\n",
    "    l_rate: float\n",
    "        The learning rate for the given loss function.\n",
    "    loss: str \n",
    "        Loss function to use, only excepts tf loss functions\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Tensorflow sequential model.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    initializer = tf.keras.initializers.GlorotNormal()\n",
    "    \n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1),padding ='same',kernel_initializer=initializer,use_bias =False))\n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1),padding ='same',kernel_initializer=initializer,use_bias =False))\n",
    "    model.add(tf.keras.layers.BatchNormalization(beta_initializer=initializer,momentum = 0.9))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1),padding ='same',kernel_initializer=initializer,use_bias =False))\n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1),padding ='same',kernel_initializer=initializer,use_bias =False))\n",
    "    model.add(tf.keras.layers.BatchNormalization(beta_initializer=initializer,momentum = 0.9))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1),padding ='same',kernel_initializer=initializer,use_bias =False))\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1),padding ='same',kernel_initializer=initializer,use_bias =False))\n",
    "    model.add(tf.keras.layers.BatchNormalization(beta_initializer=initializer,momentum = 0.9))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(256, kernel_size=(3, 3), strides=(1, 1),padding ='same',kernel_initializer=initializer,use_bias =False))\n",
    "    model.add(tf.keras.layers.Conv2D(256, kernel_size=(3, 3), strides=(1, 1),padding ='same',kernel_initializer=initializer,use_bias =False))\n",
    "    model.add(tf.keras.layers.BatchNormalization(beta_initializer=initializer,momentum = 0.9))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(1024,kernel_initializer=initializer,use_bias =False))\n",
    "    model.add(tf.keras.layers.BatchNormalization(beta_initializer=initializer,momentum = 0.9))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(1024,kernel_initializer=initializer,use_bias =False))\n",
    "    model.add(tf.keras.layers.BatchNormalization(beta_initializer=initializer,momentum = 0.9))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(1024,kernel_initializer=initializer,use_bias =False))\n",
    "    model.add(tf.keras.layers.BatchNormalization(beta_initializer=initializer,momentum = 0.9))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(output,kernel_initializer=initializer,use_bias =False))\n",
    "\n",
    "    model.compile(loss=loss,\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate = l_rate),\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#Looks for all GPUs we have access to\n",
    "device_type = 'GPU'\n",
    "devices = tf.config.experimental.list_physical_devices(\n",
    "          device_type)\n",
    "\n",
    "devices_names = [d.name.split('e:')[1] for d in devices]\n",
    "\n",
    "print('----------------------------------------------------------')\n",
    "print(devices_names)\n",
    "print('----------------------------------------------------------')\n",
    "\n",
    "\n",
    "#Enables all GPUs to be used by TF\n",
    "strategy = tf.distribute.MirroredStrategy(\n",
    "           devices=devices_names,\n",
    "           cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "\n",
    "\n",
    "#Load in dataset\n",
    "df_dataset = pd.read_pickle(\"df_recode.gzip\",compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1})\n",
    "\n",
    "#Split into training and test sets\n",
    "df_train, df_test = train_test_split(df_dataset, test_size=.2, shuffle=True, random_state=42)\n",
    "\n",
    "#Perform preprocessing\n",
    "df_train = df_train.drop(df_train[df_train['xHI'] > 0.99].index)\n",
    "df_train = df_train.drop(df_train[df_train['xHI'] < 0.01].index)\n",
    "\n",
    "df_test = df_test.drop(df_test[df_test['xHI'] > 0.99].index)\n",
    "df_test = df_test.drop(df_test[df_test['xHI'] < 0.01].index)\n",
    "\n",
    "\n",
    "#separate into images and labels\n",
    "x_train = np.array(list(df_train['maps'].values)).reshape(len(df_train),200,200,1)\n",
    "y_train = df_train['xHI'].values\n",
    "\n",
    "x_val = np.array(list(df_test['maps'].values)).reshape(len(df_test),200,200,1)\n",
    "y_val = df_test['xHI'].values\n",
    "\n",
    "\n",
    "#Standardise datasets\n",
    "means_x = np.mean(x_train)\n",
    "stds_x = np.std(x_train)\n",
    "\n",
    "x_train = (x_train - means_x)/stds_x\n",
    "x_val = (x_val - means_x)/stds_x\n",
    "\n",
    "\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "checkpoint_path = \"training/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=False,\n",
    "                                                 verbose=1)\n",
    "\n",
    "\n",
    "#model parameters\n",
    "output_nuerons = 1\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "\n",
    "#Make the model and load it onto the GPUs\n",
    "with strategy.scope():\n",
    "    model = make_model_VGG(output_nuerons,learning_rate)\n",
    "\n",
    "\n",
    "#Reshape the training and test data into TF tensors with the batch size\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "\n",
    "\n",
    "#May or maynot need this depending on GPU implementation.\n",
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "train_data = train_data.with_options(options)\n",
    "val_data = val_data.with_options(options)\n",
    "\n",
    "train_data = train_data.batch(batch_size)\n",
    "val_data = val_data.batch(batch_size)\n",
    "\n",
    "\n",
    "print('----------------------------------------------------------')\n",
    "print('                   Fitting Model                        ')\n",
    "print('----------------------------------------------------------')\n",
    "\n",
    "\n",
    "#Trains model on GPUs\n",
    "model.fit(train_data,\n",
    "              validation_data=val_data,\n",
    "              epochs=500,\n",
    "              verbose=1,\n",
    "              callbacks=[cp_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this script as ```filename_gpu.py```. With this new script you would also need a new job script to ask the HPC for GPUs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PBS -l walltime=24:00:00\n",
    "#PBS -lselect=1:ncpus=32:mem=192gb:ngpus=8:gpu_type=RTX6000\n",
    "\n",
    "module load anaconda3/personal\n",
    "\n",
    "source activate tflow\n",
    "\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "python filename_gpu.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost everything remains the same here, except the "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_recode",
   "language": "python",
   "name": "venv_recode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
